{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP를 모를 때 최고의 정책을 찾는 방법\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self): # 책에서 시작 지점을 (2, 0)으로 표기하였으나, 실제로는 (0, 0)에서 시작\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def step(self, a):\n",
    "        if a == 0:\n",
    "            self.move_left()\n",
    "        elif a == 1:\n",
    "            self.move_up()\n",
    "        elif a == 2:\n",
    "            self.move_right()\n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "\n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done # 에이전트로부터 action을 받아서 다음 상태와 보상, 에피소드가 끝났는지 여부를 리턴\n",
    "    \n",
    "    def move_left(self):\n",
    "        if self.y == 0: # x = 0에서는 left를 해도 제자리 유지\n",
    "            pass \n",
    "        elif self.y == 3 and self.x in [0, 1, 2]: # x = 3, y = 0, 1, 2에서는 left를 해도 제자리 유지 (벽이 있기 때문에)\n",
    "            pass \n",
    "        elif self.y == 5 and self.x in [2, 3, 4]: # x = 5, y = 2, 3, 4에서는 left를 해도 제자리 유지 (벽이 있기 때문에)\n",
    "            pass\n",
    "        else:\n",
    "            self.y -= 1 # 위의 경우가 아니면 left를 하면 x좌표가 1 감소\n",
    "\n",
    "    def move_right(self):\n",
    "        if self.y == 1 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.y == 6:\n",
    "            pass\n",
    "        else:\n",
    "            self.y += 1\n",
    "\n",
    "    def move_up(self):\n",
    "        if self.x == 0:\n",
    "            pass\n",
    "        elif self.y == 2 and self.x == 3:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1 # 기존대로면 += 이 맞는데, 문제 정의가 위로 갈수록 index가 작아지는 구조이기 때문에 -=로 구현\n",
    "\n",
    "    def move_down(self):\n",
    "        if self.x == 4:\n",
    "            pass\n",
    "        elif self.y == 4 and self.x == 1:\n",
    "            pass\n",
    "        else:\n",
    "            self.x += 1\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.y == 6 and self.x == 4: # Terminal State\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((5, 7, 4)) # 7 x 5 x 4 크기의 0으로 초기화된 Q 테이블 생성 -> why? -> action-value function을 표현하기 위해(4개의 액션에 대한 q값을 모두 저장하기 위해)\n",
    "        self.eps = 0.9 # 탐색을 위한 입실론 값 설정 (eps-greedy)\n",
    "        self.alpha = 0.01\n",
    "        self.gamma = 1\n",
    "\n",
    "    def select_action(self, s):\n",
    "        # eps-greedy로 액션을 선택\n",
    "        x, y = s\n",
    "        coin = random.random() # 0 ~ 1 사이의 값\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0, 3) # 0, 1, 2, 3 중 하나를 랜덤하게 선택 (90% 확률로 exploration -> 0.1까지 decaying)\n",
    "        else:\n",
    "            action_val = self.q_table[x, y, :] # 4개의 액션에 대한 q값을 모두 가져옴\n",
    "            action = np.argmax(action_val) # 10% 확률로 exploitation / 가장 큰 q값을 가지는 액션을 선택\n",
    "        return action\n",
    "    \n",
    "    def update_table(self, history):\n",
    "        # 한 에피소드에 해당하는 history를 입력으로 받아 q_table을 업데이트\n",
    "        cum_reward = 0\n",
    "        for transition in history[::-1]:\n",
    "            s, a, r, s_prime = transition\n",
    "            x, y = s\n",
    "            # 몬테카를로 방식의 업데이트\n",
    "            self.q_table[x, y, a] = self.q_table[x, y, a] + self.alpha * (cum_reward - self.q_table[x, y, a]) # q_table의 해당 위치의 q값을 업데이트\n",
    "            cum_reward = self.gamma * cum_reward + r\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.03\n",
    "        self.eps = max(self.eps, 0.1)\n",
    "\n",
    "    def show_table(self):\n",
    "        # 학습이 각 위치에서 어느 액션의 q값이 가장 높았는지 보여주는 함수\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((5, 7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2. 0. 3. 2. 2. 3.]\n",
      " [2. 3. 0. 2. 2. 2. 3.]\n",
      " [3. 3. 0. 1. 0. 2. 3.]\n",
      " [2. 3. 3. 1. 0. 3. 3.]\n",
      " [3. 2. 2. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 몬테카를로 컨트롤\n",
    "\n",
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = MCAgent()\n",
    "\n",
    "    for n_epi in range(1000):\n",
    "        done = False\n",
    "        history = []\n",
    "\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a = agent.select_action(s) # s = (0, 0), 랜덤하게 action 선택\n",
    "            s_prime, r, done = env.step(a)\n",
    "            history.append((s, a, r, s_prime))\n",
    "            s = s_prime # state 업데이트\n",
    "        agent.update_table(history) # 히스토리를 이용하여 에이전트를 업데이트\n",
    "        agent.anneal_eps() # eps 0.1까지 decaying\n",
    "\n",
    "    agent.show_table() # 학습이 끝난 결과를 출력\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD 컨트롤 1 - SARSA\n",
    "class SAgent(MCAgent):\n",
    "    def update_table(self, transition): # MC에서는 인자로 history를 받았지만, TD에서는 transition 하나만 받음\n",
    "        s, a, r, s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        a_prime = self.select_action(s_prime)\n",
    "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (r + self.gamma * self.q_table[next_x, next_y, a_prime] - self.q_table[x, y, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3. 0. 2. 2. 2. 3.]\n",
      " [3. 3. 0. 2. 2. 2. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 3.]\n",
      " [2. 2. 2. 1. 0. 3. 3.]\n",
      " [0. 1. 0. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = SAgent()\n",
    "\n",
    "    for n_epi in range(1000):\n",
    "        done = False\n",
    "\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            agent.update_table((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        agent.anneal_eps()\n",
    "\n",
    "    agent.show_table()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD 컨트롤 2 - Q-Learning\n",
    "class QAgent(MCAgent):\n",
    "    def update_table(self, transition):\n",
    "        s, a, r, s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        # Q-Learning 업데이트 식\n",
    "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (r + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, a]) # 벨만 최적 방정식을 사용\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.01 # Q-learning에서는 epsilon이 좀 더 천천히 줄어들도록 함\n",
    "        self.eps = max(self.eps, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3. 0. 2. 3. 2. 3.]\n",
      " [2. 3. 0. 2. 2. 2. 3.]\n",
      " [2. 3. 0. 1. 0. 2. 3.]\n",
      " [2. 2. 2. 1. 0. 3. 3.]\n",
      " [2. 2. 1. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "\n",
    "    for n_epi in range(1000):\n",
    "        done = False\n",
    "\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            agent.update_table((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        agent.anneal_eps()\n",
    "\n",
    "    agent.show_table()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벨만 기대 방정식과 벨만 최적 방정식이 구현된 부분 다시 이해해보기!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
