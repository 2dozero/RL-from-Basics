{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 풀고자 하는 문제인 카트폴(CartPole)은 카트를 잘 밀어서 막대가 넘어지지 않도록 균형을 잡는 문제\n",
    "- 카트는 일정한 힘으로 왼쪽이나 오른쪽으로 밀 수 있기 때문에 선택할 수 있는 액션은 항상 2가지뿐, 또한 스텝마다 +1의 보상을 받기 때문에 보상을 최적화하는 것은 곧 막대를 넘어뜨리지 않고 가능한 오래도록 균형을 잡는 것\n",
    "- 막대가 수직으로부터 15도 이상 기울어지거나 카트가 화면 끝으로 나가면 종료\n",
    "- 카트의 상태 s는 길이 4의 벡터 s = (카트의 위치, 카트의 속도, 막대의 각도, 막대의 각속도)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <구조 정리>\n",
    "- for n_epi in range(100000):\n",
    "- __(한 에피소드 동안) 각각의 state(길이 4인 벡터)의 정보를 네트워크에 전달 -> action 2개에 대한 q(s, a)값을 반환\n",
    "- __이때, epsilon-greedy하게 epsilon의 확률로는 random하게 action 선택(exploration), 나머지는 두 action중 q(s, a)값이 높은 action을 선택(exploitation)\n",
    "- __그렇게 transition 발생\n",
    "- (에피소드 종료되면) replay buffer에서 batch_size만큼의 데이터를 가지고 네트워크 update -> 어떤 state를 어떻게??\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN algorithm for CartPole 동작 과정 요약\n",
    "\n",
    "for n_epi in range(10000):\n",
    "    \n",
    "    while not done: # 한 episode 동안 ..\n",
    "        # 각각의 state(길이 4인 벡터)의 정보를 네트워크에 전달 -> action을 선택(0 or 1)\n",
    "        # action을 선택하고, 그 action을 통해 다음 state(s_prime)와 reward를 받음\n",
    "        # 해당 transition을 ReplayBuffer에 put()\n",
    "\n",
    "    if memory.size() > 2000:\n",
    "        # episode가 끝날때마다 train을 실시\n",
    "        # 즉, ReplayBuffer에서 mini_batch를 뽑아(어떤 state들???? 그냥 막?? 서로 같아..도 되나?) 해당 데이터에 대한 Loss를 계산 -> 그라디언트를 통해 Qnet에 대한 weight를 업데이트\n",
    "        # 이 과정을 10번 반복 (왜 ?>>>>)\n",
    "    \n",
    "    if n_epi%print_interval == 0 and n_epi!=0:\n",
    "        # 20 episode마다 가장 최근 20개 episode의 보상 총합의 평균을 프린트\n",
    "        # q 네트워크의 파라미터를 q_target 네트워크로 복사\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # OpenAI GYM 라이브러리\n",
    "import collections # 리플레이 버퍼를 구현할때 사용 (deque : first in first out)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하이퍼 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32 # 하나의 미니 배치 안에 32개의 데이터가 쓰인다는 의미 -> Loss function(L(seta))를 정의할때 기댓값 연산자 반드시 필요(같은 상태 s에서 같은 액션 a를 취하더라도 매번 다른 상태에 도달할 수 있기 때문에) -> 이때 사용되는 데이터의 개수가 mini batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 리플레이 버퍼 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = buffer_limit) # buffer_limit = 50,000\n",
    "\n",
    "    def put(self, transition): # 데이터를 buffer에 넣어주는 함수\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n): # buffer에서 랜덤하게 batch_size만큼의 데이터를 뽑아주는 함수\n",
    "        mini_batch = random.sample(self.buffer, n) # buffer에서 랜덤하게 n개의 데이터를 뽑아서 mini_batch에 저장(32개의 transition 데이터)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch: # transition example : [ 0.11703863  1.3383894  -0.12113763 -2.0172946 ]\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a]) # a는 스칼라 값이므로 []로 감싸줌\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype = torch.float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype = torch.float), torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q밸류 네트워크 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x= self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon): # obs : observation으로 보임(current state)\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon: # initial epsilon = 0.08\n",
    "            return random.randint(0, 1) # random하게 action 선택(exploration)\n",
    "        else:\n",
    "            return out.argmax().item() # Q value가 높은 action 선택(exploitation) : return값은 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer): # episode 하나가 끝날 때마다 train을 실시, 한 번 호출될 때마다 10개의 mini batch를 뽑아 총 10번 update (총 320개 data)\n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # q 네트워크에 state를 넣어서 q value를 구함, shape : (32, 2), first value : [0.1336,  0.1596]\n",
    "        q_a = q_out.gather(1, a) # q value 중에서 실제 취한 action에 해당하는 q value만 골라냄, shape : (32, 1), first value : [0.1336], gather(1, a)는 dim = 1(열 방향)에서 a에 해당하는 Index를 골라내는 작업\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) # q_target 네트워크에 s_prime을 넣어서 나온 q value 중에서 최대값을 골라냄, shape : (32, 1), first value : [0.1177]\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Loss function을 미분해서 각 파라미터에 대한 gradient를 계산\n",
    "        optimizer.step() # 계산된 gradient를 통해 파라미터를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 메인 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :20, score : 10.2, n_buffer : 205, eps : 7.9%\n",
      "n_episode :40, score : 10.2, n_buffer : 409, eps : 7.8%\n",
      "n_episode :60, score : 9.6, n_buffer : 601, eps : 7.7%\n",
      "n_episode :80, score : 9.6, n_buffer : 792, eps : 7.6%\n",
      "n_episode :100, score : 9.8, n_buffer : 987, eps : 7.5%\n",
      "n_episode :120, score : 9.8, n_buffer : 1183, eps : 7.4%\n",
      "n_episode :140, score : 10.0, n_buffer : 1383, eps : 7.3%\n",
      "n_episode :160, score : 9.7, n_buffer : 1577, eps : 7.2%\n",
      "n_episode :180, score : 9.4, n_buffer : 1765, eps : 7.1%\n",
      "n_episode :200, score : 9.9, n_buffer : 1963, eps : 7.0%\n",
      "n_episode :220, score : 9.7, n_buffer : 2157, eps : 6.9%\n",
      "n_episode :240, score : 9.8, n_buffer : 2353, eps : 6.8%\n",
      "n_episode :260, score : 9.9, n_buffer : 2551, eps : 6.7%\n",
      "n_episode :280, score : 9.8, n_buffer : 2747, eps : 6.6%\n",
      "n_episode :300, score : 13.7, n_buffer : 3020, eps : 6.5%\n",
      "n_episode :320, score : 20.1, n_buffer : 3423, eps : 6.4%\n",
      "n_episode :340, score : 41.0, n_buffer : 4242, eps : 6.3%\n",
      "n_episode :360, score : 71.5, n_buffer : 5672, eps : 6.2%\n",
      "n_episode :380, score : 86.8, n_buffer : 7408, eps : 6.1%\n",
      "n_episode :400, score : 132.1, n_buffer : 10050, eps : 6.0%\n",
      "n_episode :420, score : 144.7, n_buffer : 12944, eps : 5.9%\n",
      "n_episode :440, score : 158.9, n_buffer : 16122, eps : 5.8%\n",
      "n_episode :460, score : 161.8, n_buffer : 19357, eps : 5.7%\n",
      "n_episode :480, score : 183.7, n_buffer : 23031, eps : 5.6%\n",
      "n_episode :500, score : 161.8, n_buffer : 26267, eps : 5.5%\n",
      "n_episode :520, score : 167.8, n_buffer : 29623, eps : 5.4%\n",
      "n_episode :540, score : 181.8, n_buffer : 33258, eps : 5.3%\n",
      "n_episode :560, score : 181.8, n_buffer : 36893, eps : 5.2%\n",
      "n_episode :580, score : 208.3, n_buffer : 41060, eps : 5.1%\n",
      "n_episode :600, score : 186.9, n_buffer : 44799, eps : 5.0%\n",
      "n_episode :620, score : 172.1, n_buffer : 48241, eps : 4.9%\n",
      "n_episode :640, score : 192.5, n_buffer : 50000, eps : 4.8%\n",
      "n_episode :660, score : 189.8, n_buffer : 50000, eps : 4.7%\n",
      "n_episode :680, score : 206.8, n_buffer : 50000, eps : 4.6%\n",
      "n_episode :700, score : 206.9, n_buffer : 50000, eps : 4.5%\n",
      "n_episode :720, score : 205.8, n_buffer : 50000, eps : 4.4%\n",
      "n_episode :740, score : 264.7, n_buffer : 50000, eps : 4.3%\n",
      "n_episode :760, score : 237.2, n_buffer : 50000, eps : 4.2%\n",
      "n_episode :780, score : 223.5, n_buffer : 50000, eps : 4.1%\n",
      "n_episode :800, score : 197.1, n_buffer : 50000, eps : 4.0%\n",
      "n_episode :820, score : 193.8, n_buffer : 50000, eps : 3.9%\n",
      "n_episode :840, score : 191.0, n_buffer : 50000, eps : 3.8%\n",
      "n_episode :860, score : 223.2, n_buffer : 50000, eps : 3.7%\n",
      "n_episode :880, score : 207.0, n_buffer : 50000, eps : 3.6%\n",
      "n_episode :900, score : 204.7, n_buffer : 50000, eps : 3.5%\n",
      "n_episode :920, score : 179.0, n_buffer : 50000, eps : 3.4%\n",
      "n_episode :940, score : 196.9, n_buffer : 50000, eps : 3.3%\n",
      "n_episode :960, score : 223.2, n_buffer : 50000, eps : 3.2%\n",
      "n_episode :980, score : 223.8, n_buffer : 50000, eps : 3.1%\n",
      "n_episode :1000, score : 209.6, n_buffer : 50000, eps : 3.0%\n",
      "n_episode :1020, score : 182.3, n_buffer : 50000, eps : 2.9%\n",
      "n_episode :1040, score : 170.8, n_buffer : 50000, eps : 2.8%\n",
      "n_episode :1060, score : 183.2, n_buffer : 50000, eps : 2.7%\n",
      "n_episode :1080, score : 175.8, n_buffer : 50000, eps : 2.6%\n",
      "n_episode :1100, score : 186.3, n_buffer : 50000, eps : 2.5%\n",
      "n_episode :1120, score : 198.9, n_buffer : 50000, eps : 2.4%\n",
      "n_episode :1140, score : 180.3, n_buffer : 50000, eps : 2.3%\n",
      "n_episode :1160, score : 188.3, n_buffer : 50000, eps : 2.2%\n",
      "n_episode :1180, score : 202.9, n_buffer : 50000, eps : 2.1%\n",
      "n_episode :1200, score : 206.0, n_buffer : 50000, eps : 2.0%\n",
      "n_episode :1220, score : 194.3, n_buffer : 50000, eps : 1.9%\n",
      "n_episode :1240, score : 193.8, n_buffer : 50000, eps : 1.8%\n",
      "n_episode :1260, score : 178.7, n_buffer : 50000, eps : 1.7%\n",
      "n_episode :1280, score : 209.4, n_buffer : 50000, eps : 1.6%\n",
      "n_episode :1300, score : 212.3, n_buffer : 50000, eps : 1.5%\n",
      "n_episode :1320, score : 187.9, n_buffer : 50000, eps : 1.4%\n",
      "n_episode :1340, score : 185.9, n_buffer : 50000, eps : 1.3%\n",
      "n_episode :1360, score : 156.1, n_buffer : 50000, eps : 1.2%\n",
      "n_episode :1380, score : 97.6, n_buffer : 50000, eps : 1.1%\n",
      "n_episode :1400, score : 104.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1420, score : 134.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1440, score : 122.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1460, score : 96.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1480, score : 108.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1500, score : 111.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1520, score : 126.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1540, score : 129.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1560, score : 131.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1580, score : 184.8, n_buffer : 50000, eps : 1.0%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    q = Qnet() # 학습 네트워크\n",
    "    q_target = Qnet() # 타깃 네트워크\n",
    "    q_target.load_state_dict(q.state_dict()) # q 네트워크의 파라미터를 q_target 네트워크로 복사하는 과정 (이러한 update과정은 20 episode마다 실시됨)\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr = learning_rate) # update과정에서는 q 네트워크의 파라미터만 사용됨 (off-policy)\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200)) # Linear annealing from 8% to 1% (decaying epsilon greedy)\n",
    "        s, _ = env.reset()\n",
    "        # print(\"s : \", s)\n",
    "        done = False\n",
    "\n",
    "        while not done: # 한 episode 동안\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon) # a = 0 or 1\n",
    "            s_prime, r, done, truncated, info = env.step(a) \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s, a, r / 100.0, s_prime, done_mask)) # transition을 통해 얻은 데이터를 ReplayBuffer에 저장 / reward scaling -> why?\n",
    "            s = s_prime # state update\n",
    "            score += r # reward 누적 : 최근 20 episode마다 평균 reward를 프린트하기 위함(개선 여부 확인을 위해)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if memory.size() > 2000: # ReplayBuffer에 데이터가 충분히 쌓이지 않았을 때 학습을 진행하면 초기의 데이터가 많이 재사용되어 학습이 치우칠 수 있기 때문\n",
    "            train(q, q_target, memory, optimizer)\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            q_target.load_state_dict(q.state_dict()) # 20 episode마다 q 네트워크의 파라미터를 q_target 네트워크로 복사(20 episode동안 파라미터를 얼려둠)\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score / print_interval, memory.size(), epsilon * 100))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__': # 왜 n_episode 1580에서 더 출력되지 않는지 check\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 본 과정은 On-policy인가 Off-policy인가? 왜 그렇게 생각하는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (932299208.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    gym 환경은 왜 쓰는거야 ? -> state_dict()같은 내장함수를 사용하는듯, env.reset()등..reset된 상태 궁금\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gym 환경은 왜 쓰는거야 ? -> state_dict()같은 내장함수를 사용하는듯, env.reset()등..reset된 상태 궁금\n",
    "off-policy 한번 더 공부 / 체크\n",
    "그래서 결론은 ??\n",
    "왜 TD인데 episode끝날때마다 Update할까?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
